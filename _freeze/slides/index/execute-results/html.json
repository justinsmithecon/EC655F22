{
  "hash": "c06b1faf30a4b3f3220d612baf713f2c",
  "result": {
    "markdown": "---\ntitle: \"Matrix Review\"\nsubtitle: \"EC655\"\nauthor: \"Justin Smith\"\ninstitute: \"Wilfrid Laurier University\"\ndate: \"Fall 2022\"\nformat: \n  revealjs:\n    theme: [default, hygge.scss]\n    smaller: true\n    slide-number: true\n    chalkboard:\n      theme: whiteboard\n      src: drawings.json\neditor: visual\ntitle-slide-attributes:\n  data-background-color: \"#43464B\"\n---\n\n\n# Introduction\n\n## Introduction\n\n\n\n\n\n-   Undergrad metrics normally uses scalar notation\n\n    -   More accessible for students without advanced math background\n\n-   At the graduate level, it is often taught using matrix algebra\n\n-   Some advantages to matrix notation\n\n    -   More compact\n\n    -   Easier to express some estimators\n\n-   In this section, we review matrix algebra essentials for econometrics\n\n    -   Not a comprehensive review\n\n-   We will switch between scalar and matrix notation in the course\n\n    -   Depending on which is clearer in each context\n\n# Matrices and Vectors\n\n## Matrix\n\n-   A [matrix]{.red} is a rectangular array of numbers organized in rows and columns\n\n-   For example, matrix $\\mathbf{A}$ with 2 rows and 3 columns could be\n\n$$\\mathbf{A} = \n\\begin{bmatrix}\n1 & 2  & 3 \\\\\n4 &5 & 6 \n\\end{bmatrix}$$\n\n-   More generally, matrix $\\mathbf{A}$ with *m* rows and *n* columns is\n\n$$\\mathbf{A}= \n\\begin{bmatrix}\na_{11}& a_{12} &\\cdots & a_{1n} \\\\\na_{21}& a_{22} &\\cdots & a_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\na_{m1}& a_{m2} &\\cdots & a_{mn} \n\\end{bmatrix}$$\n\n## Vectors\n\n-   A [vector]{.red} is a matrix with one column or one row\n\n-   A [row vector]{.red} $\\mathbf{a}$ with *n* elements is\n\n$$\\mathbf{a}= \n\\begin{bmatrix}\na_{1}& a_{2} &\\cdots & a_{n} \n\\end{bmatrix}$$\n\n-   A .red\\[column vector\\] $\\mathbf{a}$ with *m* elements is\n\n$$\\mathbf{a}= \n\\begin{bmatrix}\na_{1}\\\\\na_{2}\\\\\n\\vdots \\\\\na_{m}\n\\end{bmatrix}$$\n\n## Special Matrices\n\n-   A [Square Matrix]{.red} has the same number of rows and columns\n\n$$\\mathbf{A}= \n\\begin{bmatrix}\na_{11}& a_{12} &\\cdots & a_{1m} \\\\\na_{21}& a_{22} &\\cdots & a_{2m} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\na_{m1}& a_{m2} &\\cdots & a_{mm} \n\\end{bmatrix}$$\n\n-   A [Diagonal Matrix]{.red} is a square matrix with zeroes for all off-diagonal elements\n\n$$\\mathbf{A}=\n\\begin{bmatrix}\na_{11}& 0&\\cdots & 0 \\\\\n0& a_{22} &\\cdots & 0 \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n0& 0&\\cdots & a_{mm} \n\\end{bmatrix}$$\n\n## Special Matrices\n\n-   The [Identity Matrix]{.red} is a square matrix with ones on the diagonal and zeroes on the off-diagonals\n\n$$\\mathbf{I}= \n\\begin{bmatrix}\n1& 0&\\cdots & 0 \\\\\n0& 1 &\\cdots & 0 \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n0& 0&\\cdots & 1 \n\\end{bmatrix}$$\n\n-   The [Zero Matrix]{.red} is a matrix with zeroes for all elements\n\n$$\\mathbf{0}= \n\\begin{bmatrix}\n0& 0&\\cdots & 0 \\\\\n0& 0 &\\cdots & 0 \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n0& 0&\\cdots & 0\n\\end{bmatrix}$$\n\n## Matrix Addition\n\n-   You can add and subtract matrices with the same dimensions\n\n    -   Matrices with different dimensions are not conformable for addition or subtraction\n\n-   The sum of matrices $\\mathbf{A}$ and $\\mathbf{B}$ with dimension $m \\times n$ is\n\n$$\\mathbf{A} + \\mathbf{B}= \n\\begin{bmatrix}\na_{11}& a_{12} &\\cdots & a_{1n} \\\\\na_{21}& a_{22} &\\cdots & a_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\na_{m1}& a_{m2} &\\cdots & a_{mn} \n\\end{bmatrix}\n+\n\\begin{bmatrix}\nb_{11}& b_{12} &\\cdots & b_{1n} \\\\\nb_{21}& b_{22} &\\cdots & b_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\nb_{m1}& b_{m2} &\\cdots & b_{mn} \n\\end{bmatrix}$$\n\n$$= \n\\begin{bmatrix}\na_{11} + b_{11}& a_{12} + b_{12} &\\cdots & a_{1n}+ b_{1n} \\\\\na_{21} + b_{21}& a_{22} + b_{22} &\\cdots & a_{2n}+ b_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\na_{m1} + b_{m1}& a_{m2} +b_{m2} &\\cdots & a_{mn}+ b_{mn} \\\\\n\\end{bmatrix}$$\n\n## Matrix Subtraction\n\n-   Similarly, the difference between matrices $\\mathbf{A}$ and $\\mathbf{B}$ is\n\n$$\\mathbf{A} - \\mathbf{B}= \n\\begin{bmatrix}\na_{11}& a_{12} &\\cdots & a_{1n} \\\\\na_{21}& a_{22} &\\cdots & a_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\na_{m1}& a_{m2} &\\cdots & a_{mn} \n\\end{bmatrix}\n-\n\\begin{bmatrix}\nb_{11}& b_{12} &\\cdots & b_{1n} \\\\\nb_{21}& b_{22} &\\cdots & b_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\nb_{m1}& b_{m2} &\\cdots & b_{mn} \n\\end{bmatrix}$$\n\n$$= \n\\begin{bmatrix}\na_{11} - b_{11}& a_{12} - b_{12} &\\cdots & a_{1n}- b_{1n} \\\\\na_{21} - b_{21}& a_{22} - b_{22} &\\cdots & a_{2n}- b_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\na_{m1} - b_{m1}& a_{m2} -b_{m2} &\\cdots & a_{mn}- b_{mn} \\\\\n\\end{bmatrix}$$\n\n## Rules for Addition and Subtraction\n\n-   The following rules apply to matrix addition and subtraction\n\n    -   Commutativity $$\\mathbf{A + B = B + A}$$\n\n    -   Associativity $$\\mathbf{A + (B + C) = (A+B) + C}$$\n\n-   Effectively, both rules mean order does not matter\n\n    -   Similar to scalar math\n\n-   For subtraction, replace plus sign with minus sign and same rules apply\n\n## Matrix Multiplication\n\n-   To multiply matrix $\\mathbf{A}$ and $\\mathbf{B}$, the number of columns in $\\mathbf{A}$ must equal the number of rows in $\\mathbf{B}$\n\n-   Suppose matrix $\\mathbf{A}$ is $m \\times n$ and matrix $\\mathbf{B}$ is $n \\times p$\n\n-   Define product as $\\mathbf{C}$= $\\mathbf{AB}$\n\n    -   The $ij$ element of $\\mathbf{C}$ is the sum of the product of the corresponding elements along the $i$th row of $\\mathbf{A}$ and $j$th column of $\\mathbf{B}$\\\n        $$c_{ij} = \\sum_{k} a_{ik}b_{kj}$$\n\n    -   The product matrix $\\mathbf{C}$ will have dimension $m \\times p$\n\n        -   The number of rows of $\\textbf{A}$ and number of columns of $\\textbf{B}$\n\n## Matrix Multiplication\n\n-   The product $\\mathbf{AB}$ is\n\n$$\\mathbf{AB}= \n    \\begin{bmatrix}\n    a_{11}& a_{12} &\\cdots & a_{1n} \\\\\n    a_{21}& a_{22} &\\cdots & a_{2n} \\\\\n    \\vdots & \\vdots &\\ddots & \\vdots \\\\\n    a_{m1}& a_{m2} &\\cdots & a_{mn} \n    \\end{bmatrix}\n    \\times\n    \\begin{bmatrix}\n    b_{11}& b_{12} &\\cdots & b_{1p} \\\\\n    b_{21}& b_{22} &\\cdots & b_{2p} \\\\\n    \\vdots & \\vdots &\\ddots & \\vdots \\\\\n    b_{n1}& b_{n2} &\\cdots & b_{np} \n    \\end{bmatrix}$$\n\n::: footnotesize\n$$= \n\\begin{bmatrix}\na_{11} b_{11} + a_{12} b_{21}  + \\cdots + a_{1n} b_{n1}  &a_{11} b_{12} + a_{12} b_{22}  + \\cdots + a_{1n} b_{n2} &\\cdots&a_{11} b_{1p} + a_{12} b_{2p}  + \\cdots + a_{1n} b_{np}\\\\\na_{21} b_{11} + a_{22} b_{21}  + \\cdots + a_{2n} b_{n1}  &a_{21} b_{12} + a_{22} b_{22}  + \\cdots + a_{2n} b_{n2} &\\cdots&a_{21} b_{1p} + a_{22} b_{2p}  + \\cdots + a_{2n} b_{np}\\\\\n\\vdots &\\ddots & \\vdots \\\\\na_{m1} b_{11} + a_{m2} b_{21}  + \\cdots + a_{mn} b_{n1}  &a_{m1} b_{12} + a_{m2} b_{22}  + \\cdots + a_{mn} b_{n2} &\\cdots&a_{m1} b_{1p} + a_{m2} b_{2p}  + \\cdots + a_{mn} b_{np}\\\\\n\\end{bmatrix}$$\n:::\n\n## Matrix Multiplication\n\n-   As an illustration suppose we have the following matrices $$\\mathbf{A}=\n    \\begin{bmatrix}\n    1& 2\\\\\n    3& 4 \\\\\n    \\end{bmatrix}\n    \\mathbf{B}=\n    \\begin{bmatrix}\n    5&6&7  \\\\\n    8&9 &10 \n    \\end{bmatrix}$$\n\n-   We can multiply $\\mathbf{AB}$ because $\\mathbf{A}$ has 2 columns, and $\\mathbf{B}$ has 2 rows\n\n-   The product $\\mathbf{C}$ = $\\mathbf{AB}$ is\n\n$$\\mathbf{C}= \n\\begin{bmatrix}\n1& 2\\\\\n3& 4 \\\\\n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\n5&6&7  \\\\\n8&9 &10 \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\times 5 + 2\\times 8&1 \\times 6 + 2 \\times 9 & 1 \\times 7 + 2 \\times 10  \\\\\n3 \\times 5 + 4\\times 8&3 \\times 6 + 4 \\times 9 & 3 \\times 7 + 4 \\times 10  \n\\end{bmatrix}$$\n\n$$=\n\\begin{bmatrix}\n21& 24& 27 \\\\\n47&54&  61 \n\\end{bmatrix}$$\n\n## Scalar Multiplication\n\n-   A [scalar]{.red} is a single real number\n\n-   You can also multiply a scalar by a matrix\n\n-   If $\\gamma$ is a scalar, and $\\mathbf{A}$ is a matrix, then\n\n$$\\mathbf{\\gamma A}= \\gamma \n\\begin{bmatrix}\na_{11}& a_{12} &\\cdots & a_{1n} \\\\\na_{21}& a_{22} &\\cdots & a_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\na_{m1}& a_{m2} &\\cdots & a_{mn} \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\gamma a_{11}&\\gamma  a_{12} &\\cdots & \\gamma a_{1n} \\\\\n\\gamma a_{21}& \\gamma a_{22} &\\cdots & \\gamma a_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n\\gamma a_{m1}& \\gamma a_{m2} &\\cdots & \\gamma a_{mn} \n\\end{bmatrix}$$\n\n-   You multiply the scalar by each element of the matrix\n\n## Transpose\n\n-   The [transpose]{.red} of a matrix is one where the rows and columns are switched\n\n-   Suppose matrix $\\mathbf{A}$ is\n\n$$\\mathbf{A}= \n    \\begin{bmatrix}\n    a_{11}& a_{12} &\\cdots & a_{1n} \\\\\n    a_{21}& a_{22} &\\cdots & a_{2n} \\\\\n    \\vdots & \\vdots &\\ddots & \\vdots \\\\\n    a_{m1}& a_{m2} &\\cdots & a_{mn} \n    \\end{bmatrix}$$\n\n-   Then its transpose $\\mathbf{A'}$ is\n\n$$\\mathbf{A'}= \n\\begin{bmatrix}\na_{11}& a_{21} &\\cdots & a_{m1} \\\\\na_{12}& a_{22} &\\cdots & a_{m2} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\na_{1n}& a_{2n} &\\cdots & a_{mn} \n\\end{bmatrix}$$\n\n## Transpose {#transpose-1}\n\n-   The transpose has the following properties\n\n$$\\mathbf{(A')' = A }$$ $$\\mathbf{(\\alpha A)' = \\alpha A' }$$ $$\\mathbf{(A + B)' = A' + B' }$$ $$\\mathbf{(AB)' = B'A' }$$\n\n-   There are additional rules for different types of matrices that we will cover below\n\n## Partitioned Matrix Multiplication\n\n-   You may sometimes want to break matrices into vectors before you multiply\n\n-   Multiplication works the same way, but notation can be cleaner and more intuitive\n\n-   Suppose we have the following matrices $$\\mathbf{A}= \n    \\begin{bmatrix}\n    a_{11}& a_{12} &\\cdots & a_{1n} \\\\\n    a_{21}& a_{22} &\\cdots & a_{2n} \\\\\n    \\vdots & \\vdots &\\ddots & \\vdots \\\\\n    a_{m1}& a_{m2} &\\cdots & a_{mn} \n    \\end{bmatrix}\n    \\mathbf{B}= \n    \\begin{bmatrix}\n    b_{11}& b_{12} &\\cdots & b_{1p} \\\\\n    b_{21}& b_{22} &\\cdots & b_{2p} \\\\\n    \\vdots & \\vdots &\\ddots & \\vdots \\\\\n    b_{n1}& b_{n2} &\\cdots & b_{np} \n    \\end{bmatrix}$$\n\n-   We are interested in the product $\\mathbf{AB}$\n\n## Partitioned Matrix Multiplication\n\n-   Break these matrices into vectors conformable for multiplication\n\n$$\\mathbf{A}=\n\\begin{bmatrix}\n\\mathbf{a_{1}}&\\mathbf{a_{2}} & \\cdots & \\mathbf{a_{n}}\n\\end{bmatrix}\n\\mathbf{B}= \n\\begin{bmatrix}\n\\mathbf{b_{1}}\\\\\n\\mathbf{b_{2} }\\\\\n\\vdots  \\\\\n\\mathbf{b_{n}}\n\\end{bmatrix}$$\n\n-   Where\n\n$$\\mathbf{a_{1}}=\n\\begin{bmatrix}\na_{11}\\\\\na_{21}\\\\\n\\cdots\\\\\na_{m1}\n\\end{bmatrix}\n\\mathbf{b_{1}}=\n\\begin{bmatrix}\nb_{11}&b_{12} & \\cdots & b_{1p}\n\\end{bmatrix}$$\n\n## Partitioned Matrix Multiplication\n\n-   Multiply the vectors to get\n\n$$\\mathbf{AB} = \\sum_{i=1}^{n} \\mathbf{a_{i}b_{i}}$$\n\n-   This breaks the product $\\mathbf{AB}$ into the sum of $n$ sub-matrices\n\n    -   Each sub-matrix is product of corresponding vectors\n\n    -   Also each sub-matrix will have dimension $m \\times p$\n\n-   This will be useful for some econometric estimators we derive\n\n    -   Makes notation simpler and more intuitive\n\n-   Again, note that you get the same answer as doing straight matrix multiplication\n\n## Rules for Matrix Multiplication\n\n-   There are several useful properties for matrix (and scalar) multiplication\n\n$$(\\alpha + \\beta)\\mathbf{A} = \\alpha \\mathbf{A} + \\beta\\mathbf{A}$$ $$\\alpha (\\mathbf{A} +\\mathbf{B}) =\\alpha \\mathbf{A} +\\alpha\\mathbf{B}$$ $$(\\alpha\\beta) \\mathbf{A}  =\\alpha(\\beta \\mathbf{A})$$ $$\\alpha (\\mathbf{A}\\mathbf{B}) =(\\alpha \\mathbf{A}) \\mathbf{B}$$ $$(\\mathbf{A}\\mathbf{B} )\\mathbf{C} =\\mathbf{A}(\\mathbf{B} \\mathbf{C})$$ $$\\mathbf{A}(\\mathbf{B} +\\mathbf{C}) =\\mathbf{A}\\mathbf{B} +\\mathbf{A} \\mathbf{C}$$ $$(\\mathbf{A}+\\mathbf{B} )\\mathbf{C} =\\mathbf{A}\\mathbf{C} +\\mathbf{B} \\mathbf{C}$$ $$\\mathbf{A}\\mathbf{I}  =\\mathbf{I}\\mathbf{A} = \\mathbf{A}$$ $$\\mathbf{A}\\mathbf{0}  =\\mathbf{0}\\mathbf{A} = \\mathbf{0}$$ $$\\mathbf{A}\\mathbf{B}  \\neq\\mathbf{B}\\mathbf{A}$$ ---\n\n## Trace\n\n-   The [trace]{.red} of a square matrix is the sum of the diagonal elements\n\n-   If square matrix $\\mathbf{A}$ is\n\n$$\\mathbf{A}= \n    \\begin{bmatrix}\n    a_{11}& a_{12} &\\cdots & a_{1n} \\\\\n    a_{21}& a_{22} &\\cdots & a_{2n} \\\\\n    \\vdots & \\vdots &\\ddots & \\vdots \\\\\n    a_{n1}& a_{n2} &\\cdots & a_{nn} \n    \\end{bmatrix}$$\n\n-   Then its trace is\n\n$$tr(\\mathbf{A})= \\sum_{i=1}^{n} a_{ii}$$\n\n## Trace {#trace-1}\n\n-   Important properties of the trace are\n\n$$tr(\\mathbf{I_{n}})= n$$ $$tr(\\mathbf{A}')=tr(\\mathbf{A})$$ $$tr(\\mathbf{A +B})=tr(\\mathbf{A}) + tr(\\mathbf{B})$$ $$tr(\\alpha \\mathbf{A})=\\alpha tr(\\mathbf{A})$$ $$tr(\\mathbf{AB})=tr(\\mathbf{BA})$$\n\n## Marix Determinant\n\n-   The [determinant]{.red} is a scalar value associated with a square matrix\n\n    -   Helpful concept for several things in matrix algebra\n\n    -   For econometrics, most useful for solving systems of equations and finding inverse of a matrix\n\n-   For $2 \\times 2$ matrix $\\mathbf{A}$ $$\\mathbf{A}= \n    \\begin{bmatrix}\n    a_{11}& a_{12} \\\\\n    a_{21}& a_{22}  \\\\\n    \\end{bmatrix}$$\n\n-   The determinant is\n\n$$|\\mathbf{A}|=a_{11}a_{22} - a_{12}a_{21}$$\n\n## Marix Determinant\n\n-   For $3 \\times 3$ matrix $\\mathbf{A}$\n\n$$\\mathbf{A}= \n\\begin{bmatrix}\na_{11}& a_{12} & a_{13} \\\\\na_{21}& a_{22} & a_{23} \\\\\na_{31}& a_{32} & a_{33} \\\\\n\\end{bmatrix}$$\n\n-   The determinant is\n\n$$|\\mathbf{A}|=a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} +a_{13}a_{21}a_{32}$$ $$-(a_{12}a_{21}a_{33} + a_{11}a_{23}a_{32} +a_{13}a_{22}a_{31})$$\n\n$$=a_{11}(a_{22}a_{33} - a_{23}a_{32}) + a_{12}(a_{23}a_{31} -a_{21}a_{33} )  +a_{13}(a_{21}a_{32} - a_{22}a_{31} )$$\n\n## Marix Determinant\n\n-   For $n \\times n$ matrix $\\mathbf{A}$ the determinant is\n\n$$|\\mathbf{A}|=a_{i1}c_{i1} + a_{i2}c_{i2} + \\cdots + a_{in}c_{in} \\text{   for choice of any row i}$$\n\n-   Where\n\n    -   $a_{ij}$ is the $ij$ element of matrix $\\mathbf{A}$\n\n    -   $c_{ij}$ is the $ij$ [cofactor]{.red} of matrix $\\mathbf{A}$ defined as $$c_{ij} = (-1)^{i+j}|\\mathbf{A}_{ij}|$$\n\n    -   $|\\mathbf{A}_{ij}|$ is the [minor]{.red} of matrix $\\mathbf{A}$\n\n        -   Determinant of the sub-matrix formed by deleting the $i$th row and $j$th column of $\\mathbf{A}$\n\n-   Process is long and tedious for large matrices\n\n## Marix Determinant\n\n-   Example of $3 \\times 3$ matrix\n\n$$\\mathbf{A}= \n\\begin{bmatrix}\n1& 2 & 3 \\\\\n4& 5&6   \\\\\n7& 8 &9  \n\\end{bmatrix}$$\n\n-   Choose any row to find cofactors and compute determinant\n\n    -   Does not matter which\n\n-   Let us expand along row 1\n\n$$|\\mathbf{A}|=1(-1)^{1+1} \n\\begin{vmatrix}\n  5&6   \\\\\n 8 &9  \n\\end{vmatrix}\n+2(-1)^{1+2} \n\\begin{vmatrix}\n  4&6   \\\\\n 7 &9  \n\\end{vmatrix}\n+3(-1)^{1+3} \n\\begin{vmatrix}\n  4&5   \\\\\n 7 &8  \n\\end{vmatrix}$$\n\n$$|\\mathbf{A}|= -3 +12 -9 = 0$$\n\n## Matrix Inverse\n\n-   The [inverse]{.red} of a square matrix $\\mathbf{A}$ is defined such that\n\n$$\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$$\n\n-   It is roughly the equivalent of taking the reciprocal in scalar math\n\n    -   But it is **not** generally the reciprocal of the elements of a matrix\n\n-   The formula for the inverse is\n\n$$\\mathbf{A}^{-1}= \\frac{1}{|\\mathbf{A}|}\n\\begin{bmatrix}\nc_{11}& c_{12} &\\cdots & c_{1n} \\\\\nc_{21}& c_{22} &\\cdots & c_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\nc_{n1}& c_{n2} &\\cdots & c_{nn} \n\\end{bmatrix}$$\n\n-   where $c_{ij}$ are the cofactors defined above\n\n## Matrix Inverse\n\n-   The inverse exists only when $|\\mathbf{A}| \\neq 0$\n\n    -   This is why it is important to know the determinant\n\n    -   In example above, inverse does not exist\n\n        -   We will see later that it is because the columns are linearly dependent\n\n-   A matrix that cannot be inverted is [singular]{.red}\n\n-   A matrix that has an inverse is [nonsingular]{.red}\n\n-   Inverse matrices have the following properties\n\n$$\\mathbf{(\\alpha A)^{-1} = \\frac{1}{\\alpha} A^{-1} }$$ $$\\mathbf{(A')^{-1}} = \\mathbf{(A^{-1})' }$$ $$\\mathbf{(A^{-1})^{-1}} = \\mathbf{A}$$ $$\\mathbf{(AB)^{-1}= B^{-1}A^{-1} }$$\n\n# Linear Independence and Rank of a Matrix\n\n## Summary\n\n-   Now that we can manipulate matrices, we can move to more advanced topics\n\n-   Matrix algebra is useful for expressing and solving systems of equations\n\n    -   This is how we will use it in econometrics\n\n-   We will learn you can solve for the OLS estimator when regressors are linearly independent\n\n    -   They are not linear functions of one another\n\n-   To check linear independence, we use the concept of rank\n\n-   The [rank]{.red} of a matrix is the maximum number of independent rows or columns\n\n    -   For non-square matrices, the maximum rank is the lesser of the number or rows or columns\n\n## Linear Independence\n\n-   A set of vectors are [linearly independent]{.red} if you cannot express any of them as linear functions the others\n\n-   Mathematically, suppose that $\\mathbf{A}=\\begin{bmatrix} \\mathbf{a}_{1}& \\mathbf{a}_{2} &\\cdots & \\mathbf{a}_{m} \\end{bmatrix}$\n\n    -   where $\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\cdots,\\mathbf{a}_{m}$ are $n \\times 1$ vectors\n\n-   The vectors are independent if the only solution to\n\n$$\\alpha_{1}\\mathbf{a}_{1}+ \\alpha_{2}\\mathbf{a}_{2}+ \\cdots+\\alpha_{m}\\mathbf{a}_{m}= 0$$\n\n-   is\n\n$$\\alpha_{1} = \\alpha_{2}= \\cdots=\\alpha_{m}= 0$$ - If at least one $\\alpha_{i} \\neq 0$, then the vectors are linearly dependent\n\n## Rank of a Matrix\n\n-   The [rank]{.red} of a matrix is the maximum number of linearly independent rows or columns\n\n    -   The rank of the rows will always equal the rank of the columns\n\n    -   If the number of rows is less than columns, the highest rank is the number of rows\n\n    -   Vice versa if the number of columns is less than the number of rows\n\n-   A matrix has [full rank]{.red} if rank equals the minimum of the number of rows/columns\n\n-   In econometrics, we mostly deal with matrices with more rows than columns\n\n    -   So the matrix will be full rank if the rank equals the number of columns\n\n-   We will see later we need our matrix of regressors to have full rank\n\n    -   None of the regressors can be linear functions of each other (no multicollinearity)\n\n## Rank of a Matrix\n\n-   Some useful properties of the rank of a matrix\n\n    -   The rank of a matrix and transpose are the same $$rank(\\mathbf{A'}) = rank(\\mathbf{A})$$\n\n    -   If $\\mathbf{A}$ is $n \\times m$ then $$rank(\\mathbf{A}) \\le min(n,m)$$\n\n    -   If $\\mathbf{A}$ is $n \\times n$ and $rank(\\mathbf{A}) =n$ then $\\mathbf{A}$ is nonsingular (invertible)\n\n# Quadratic Forms and Positive Definite Matrices\n\n## Quadratic Form\n\n-   If $\\mathbf{A}$ is $n \\times n$ and symmetric, and $\\mathbf{x}$ is $n \\times 1$, the [quadratic form]{.red} for $\\mathbf{A}$ is\n\n$$\\mathbf{x'Ax}=\n\\begin{bmatrix}\nx_{1}& x_{2} &\\cdots & x_{n} \n\\end{bmatrix}\n\\begin{bmatrix}\na_{11}& a_{12} &\\cdots & a_{1n} \\\\\na_{21}& a_{22} &\\cdots & a_{2n} \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\na_{n1}& a_{n2} &\\cdots & a_{nn} \n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1}\\\\\n x_{2} \\\\\n \\vdots \\\\\n  x_{n} \n\\end{bmatrix}$$\n\n$$=\\sum_{i=1}^n a_{ii}x_{i}^2 + 2\\sum_{i=1}^n \\sum_{j>i}a_{ij}x_{i}x_{j}$$\n\n-   A matrix is [positive definite]{.red} if for all $\\mathbf{x} \\neq 0$\n\n$$\\mathbf{x'Ax} > 0$$\n\n## Positive Definite Matrices\n\n-   A matrix is [positive semidefinite]{.red} if for all $\\mathbf{x} \\neq 0$\n\n$$\\mathbf{x'Ax} \\ge 0$$ - Positive definite matrices have diagonal elements that are strictly positive\n\n-   Positive semidefinite matrices have diagonal elements that are nonnegative\n\n-   Some other useful properties of positive definite/semidefinite matrices\n\n    -   If $\\mathbf{A}$ is positive definite, then $\\mathbf{A}^{-1}$ exists and is also positive definite\n\n    -   If $\\mathbf{A}$ is $n \\times m$, then $\\mathbf{A'A}$ and $\\mathbf{AA'}$ are positive definite\n\n    -   If $\\mathbf{A}$ is $n \\times m$ and $rank(\\mathbf{A}) = m$ then $\\mathbf{A'A}$ is positive definite\n\n-   These concepts are used mostly for variance-covariance matrices in econometrics\n\n## Idempotent Matrices\n\n-   An [idempotent]{.red} matrix is one that does not change when multiplied by itself\n\n-   Mathematically, $\\mathbf{A}$ is idempotent when\n\n$$\\mathbf{AA} = \\mathbf{A}$$\n\n-   When we discuss OLS, we will work with the following idempotent matrices\n\n    -   Suppose $\\mathbf{X}$ is $n \\times k$ with full rank. Define\n\n$$\\mathbf{P} = \\mathbf{X(X'X)^{-1}X'}$$ $$\\mathbf{M} =\\mathbf{I_{n}} - \\mathbf{X(X'X)^{-1}X'}$$\n\n-   You can verify they are idempotent my multiplying each by itself\n\n-   Some important properties of idempotent matrices are\n\n    -   $rank(\\mathbf{A}) = tr(\\mathbf{A})$\n\n    -   $\\mathbf{A}$ is positive semidefinite\n\n# Moments of Random Vectors\n\n## Expected Value\n\n-   The [expected value]{.red} of a random matrix is the matrix of expected values\n\n-   If $\\mathbf{X}$ is an $n \\times m$ matrix, then\n\n$$\\mathbf{E}(\\mathbf{X})=\n\\begin{bmatrix}\n\\mathbf{E}(x_{11}) & \\mathbf{E}(x_{12}) & \\cdots & \\mathbf{E}(x_{1m})\\\\\n\\mathbf{E}(x_{21}) & \\mathbf{E}(x_{22}) & \\cdots &\\mathbf{E}(x_{2m})\\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n\\mathbf{E}(x_{n1}) & \\mathbf{E}(x_{n2}) & \\cdots &\\mathbf{E}(x_{nm})\\\\\n\\end{bmatrix}$$\n\n-   Properties of expected values are similar to those in scalar math\n\n    -   If $\\mathbf{x}$ is a random vector, $\\mathbf{b}$ is a nonrandom vector, and $\\mathbf{A}$ is a nonrandom matrix, then $\\mathbf{E}(\\mathbf{Ax+b}) = \\mathbf{A}\\mathbf{E}(\\mathbf{x})+\\mathbf{b}$\n\n    -   If $\\mathbf{X}$ is a random matrix, and $\\mathbf{B}$ and $\\mathbf{A}$ are nonrandom matrices, then $\\mathbf{E}(\\mathbf{AXB}) = \\mathbf{A}\\mathbf{E}(\\mathbf{X})\\mathbf{B}$\n\n## Variance-Covariance Matrix\n\n-   The [variance-covariance matrix]{.red} of random vector $\\mathbf{y}$ has variances on the diagonal, covariances in the off-diagonal\n\n-   If $\\mathbf{y}$ is an $n \\times 1$ random vector, then\n\n$$var(\\mathbf{y})= \\mathbf{\\sigma_{y}} = \\mathbf{E[(y-E[y])(y-E[y])']}$$ $$=\n\\begin{bmatrix}\n\\text{var}(y_{1}) & \\text{cov}(y_{1},y_{2}) & \\cdots &\\text{cov}(y_{1},y_{n}) \\\\\n\\text{cov}(y_{2},y_{1}) & \\text{var}(y_{2}) & \\cdots &\\text{cov}(y_{2},y_{n}) \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n\\text{cov}(y_{n},y_{1})  & \\text{cov}(y_{n},y_{2}) & \\cdots &\\text{var}(y_{n})\\\\\n\\end{bmatrix}$$\n\n## Variance-Covariance Matrix {#variance-covariance-matrix-1}\n\n-   Useful properties of variance-covariance matrices are\n\n    -   If $\\mathbf{a}$ is a nonrandom vector, then $\\text{var}(\\mathbf{a'y}) =\\mathbf{a'}\\text{var}\\mathbf{(y)a}$\n\n    -   If $\\text{var}(\\mathbf{a'y})>0$ for all $\\mathbf{a>0}$, $\\text{var}(\\mathbf{y})$ is positive definite\n\n    -   If $\\mathbf{A}$ is a nonrandom matrix, $\\mathbf{b}$ is a nonrandom vector, then $\\text{var}(\\mathbf{Ay + b}) =\\mathbf{A'}\\text{var}\\mathbf{(y)A}$\n\n    -   If $\\text{var}(y_{j})=\\sigma^{2}$ for all $j=1,2,...,n$, and the elements of $\\textbf{y}$ are uncorrelated, then $\\text{var}(\\mathbf{y})=\\sigma^{2}\\mathbf{I_{n}}$\n\n# Matrix Differentiation\n\n## Scalar Functions\n\n-   A scalar function of a vector is a single function with respect to several variables\n\n    -   A vector function is a set of one or more scalar functions, each with respect to several variables\n\n    -   We will not cover these\n\n-   Consider the scalar function $y = f(\\mathbf{x}) =f(x_{1}, x_{2},...,x_{n})$\n\n    -   The function takes the vector $\\mathbf{x}$ and returns a scalar\n\n    -   This is just another way to write a multivariate function\n\n-   The derivative of this function is\n\n$$\\frac{\\partial f(\\mathbf{x})}{\\mathbf{x}}=\n\\begin{bmatrix}\n\\frac{\\partial f(\\mathbf{x})}{x_{1}} & \\frac{\\partial f(\\mathbf{x})}{x_{2}} & \\cdots & \\frac{\\partial f(\\mathbf{x})}{x_{n}}  \n\\end{bmatrix}$$\n\n## Derivative of Scalar Function\n\n-   We simply collect the derivative with respect to each element of $\\mathbf{x}$ in a vector\n\n-   Ex: linear function of $\\mathbf{x}$\n\n    -   Suppose $\\mathbf{a}$ is an $n \\times 1$ vector and $$y = f(\\mathbf{x}) = \\mathbf{a'x} = \\sum_{i=1}^{n} a_{i}x_{i}$$\n\n    -   The derivative is\n\n    $$\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}=\\frac{\\partial \\mathbf{a'x} }{\\partial \\mathbf{x}}= \\mathbf{a'} =\n        \\begin{bmatrix}\n        a_{1}& a_{2}& \\cdots & a_{n}\n        \\end{bmatrix}$$\n\n## Derivative of Scalar Function\n\n-   Ex: Quadratic form of $\\mathbf{x}$\n\n    -   Suppose $\\mathbf{A}$ is an $n \\times n$ symmetric matrix. The quadratic form is $$y = f(\\mathbf{x}) = \\mathbf{x'Ax} =\\sum_{i=1}^n a_{ii}x_{i}^2 + 2\\sum_{i=1}^n \\sum_{j>i}a_{ij}x_{i}x_{j}$$\n\n    -   The derivative is $$\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}=\\frac{\\partial \\mathbf{x'Ax} }{\\partial \\mathbf{x}}= \\mathbf{2x'A}$$\n\n# Linear Regression Model in Matrix Notation\n\n## Population Regression Model\n\n-   In undergraduate textbooks, the population linear regression model is written as\n\n$$y= \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\cdots + \\beta_{k}x_{k} + u$$\n\n-   $y$ and $x_{1},...,x_{k}$ are observable random variables\n\n-   $u$ is an unobservable random variable\n\n-   We can write more compactly in vector form as\n\n$$y=  \\mathbf{x}\\boldsymbol{\\beta}  + u$$\n\n-   $\\mathbf{x}$ is a $1 \\times (k+1)$ vector of independent variables\n\n    -   There are $k$ independent variables, plus an intercept\n\n-   $\\boldsymbol{\\beta}$ is a $(k+1) \\times 1$ vector of slope parameters\n\n## Population Regression Model\n\n-   Now suppose we take a random sample of $n$ people from the population\n\n-   The population model holds for each member of the sample\n\n$$y_{i}=  \\mathbf{x_{i}}\\boldsymbol{\\beta}  + u_{i}, \\forall i=1,...,n$$\n\n-   We can express this more compactly with full matrix notation\n\n$$\\mathbf{y}=  \\mathbf{X}\\boldsymbol{\\beta}  + \\mathbf{u}$$\n\n-   $\\mathbf{X}$ is an $n \\times (k+1)$ matrix of observations on each regressor\n\n-   $\\boldsymbol{\\beta}$ is still a $(k+1) \\times 1$ vector of slope parameters\n\n-   $\\mathbf{y}$ is an $n \\times 1$ vector of observations on the dependent variable\n\n-   $\\mathbf{u}$ is an $n \\times 1$ vector of error terms\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}