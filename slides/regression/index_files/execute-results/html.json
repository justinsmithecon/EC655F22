{
  "hash": "1a191feb646867aafef5f5863a080097",
  "result": {
    "markdown": "---\nformat: \n  revealjs:\n    theme: [default, hygge.scss]\n    center-title-slide: false\n    slide-number: true\n    height: 900\n    width: 1600\n    chalkboard:\n      theme: whiteboard\n      src: drawings.json\neditor: visual\n---\n\n\n<h1>Conditional Expectation Functions and Regression</h1>\n\n<hr>\n\n<h2>EC655 - Econometrics</h2>\n\n<h2>Justin Smith</h2>\n\n<h2>Wilfrid Laurier University</h2>\n\n<h2>Fall 2023</h2>\n\n![](/files/img/hexEC655.png){.absolute top=\"300\" left=\"900\" width=\"550\"}\n\n# Introduction\n\n## Introduction\n\n-   Questions in economics often involve explaining a variable in terms of others\n\n    -   Does age of school entry affect test scores?\n\n    -   Does childhood health insurance affect adult health?\n\n    -   Does foreign competition affect domestic innovation?\n\n-   Interest is usually in the *causal* relationship\n\n    -   The *independent* effect of one variable on another\n\n-   Econometrics provides a framework for examining these relationships\n\n    -   Strong focus on causality\n\n# What Are We Trying To Model?\n\n## Conditional Expectation Function\n\n-   We want to relate dependent variable $y$ to independent variables $\\mathbf{x}$\n\n-   Want to know *systematically* what happens to $y$ when $\\mathbf{x}$ changes\n\n-   Difficult because $y$ and $\\mathbf{x}$ are random variables\n\n    -   $y$ can take many different values for each $\\mathbf{x}$\n\n-   A way systematic patterns is to focus on average $y$ at each $\\mathbf{x}$\n\n    -   Ex: Does average achievement fall as we increase class size?\n\n-   This is the conditional expectation function (CEF) $\\mathbf{E}[y|\\mathbf{x}]$\n\n::: callout-note\nThe CEF is the population average value of $y$ at each $\\mathbf{x}$. The average can change at different $\\mathbf{x}$, meaning it is a function of $\\mathbf{x}$.\n:::\n\n## Conditional Expectation Function\n\n::: columns\n::: {.column width=\"50%\"}\n-   Log earnings on vertical axis, years of schooling on horizontal\n\n-   Grey shaded areas are distribution of log earnings at each level of schooling\n\n    -   Big spread incomes for each level of schooling\n\n-   Black line is the CEF of earnings at each level of schooling\n\n    -   Increasing pattern between school and earnings is easier to see\n:::\n\n::: {.column width=\"50%\"}\n![](CEF.png){fig-align=\"center\"}\n:::\n:::\n\n## Conditional Expectation Function\n\n-   The CEF highlights the pattern through randomness\n\n-   It is the optimal predictor of $y$ given $\\mathbf{x}$\n\n    -   It minimizes the mean squared error in predicting $y$\n\n-   Problem with using CEF: as a population value, it is not known\n\n    -   It is not observable because we do not see the population\n\n-   Instead, use linear regression to approximate it\n\n# Linear Regression\n\n## Motivation\n\n-   We can use linear regression to approximate CEF. Why?\n\n    -   If the CEF *is* linear, it is equivalent to population regression function\n\n    -   The population regression function is the best linear predictor of $y$ given $\\mathbf{x}$\n\n    -   The population regression function is the best linear approximation to the CEF\n\n-   This is partly why linear regression is popular in economics\n\n-   Next section examines **population regression**\n\n    -   We will cover estimation of the population regression with a sample later\n\n::: callout-note\nMost undergrad classes do not derive the population regression slope and instead skip directly to estimation with a sample, so this may be new. It is important to understand that at this point there is no data; we are only talking about features of the population. As you will see later, the population and sample regression functions are closely related.\n:::\n\n## Population Regression Function\n\n-   A linear model relating $y$ to explanatory variables $\\mathbf{x}$ is\n\n$$y = \\mathbf{x}\\boldsymbol{\\beta} + u$$\n\n-   Where\n\n    -   $y$ is a scalar observable random outcome variable\n\n    -   $\\mathbf{x}$ is a $1\\times (k + 1)$ vector of random explanatory factors\n\n    -   $\\boldsymbol{\\beta}$ is a $(k + 1) \\times 1$ vector of slope parameters (non-random)\n\n    -   $u$ is a scalar population residual term\n\n-   $\\mathbf{x}\\boldsymbol{\\beta}$ is called the **Population Regression Function (PRF)**\n\n    -   The part of $y$ that is predictable by $\\mathbf{x}$\n\n## Population Regression Function\n\n-   Use the PRF to approximate the CEF\n\n-   If CEF is linear, PRF equals CEF\n\n    -   True when model is \"saturated\" or when variables are joint Normal\n\n-   Still useful to use PRF if CEF is not linear\n\n    -   Goal is capture essential features of relationship\n\n::: callout-tip\n## Saturated Models\n\nA saturated model is one where the independent variables are discrete, and there is a dummy variable for each possible value it can take. For example if you regress wages on gender, a (saturated) CEF is\n\n$$E[wage|female] = \\alpha + \\beta female$$\n\nwhere $\\alpha = E[wage|female = 0]$ and $\\beta =E[wage|female = 1] - E[wage|female = 0]$\n:::\n\n## Population Regression Slope Vector\n\n-   The **population least squares vector** minimizes the mean squared prediction error (MSPE)\n\n$$\\min_\\beta \\textbf{E}[(y-\\mathbf{x}\\boldsymbol{\\beta})^2]$$\n\n-   Take the derivative with respect to $\\boldsymbol{\\beta}$ to get first order condition\n\n$$\\textbf{E}[\\mathbf{x}'(y-\\mathbf{x}\\boldsymbol{\\beta})]= \\mathbf{0}$$\n\n-   Solve for $\\boldsymbol{\\beta}$\n\n$$\\textbf{E}[\\mathbf{x}'y]= \\textbf{E}[\\mathbf{x'x}\\boldsymbol{\\beta}]$$ $$\\textbf{E}[\\mathbf{x}'y]= \\textbf{E}[\\mathbf{x'x}]\\boldsymbol{\\beta}$$ $$(\\textbf{E}[\\mathbf{x'x}])^{-1} \\textbf{E}[\\mathbf{x}'y]= \\boldsymbol{\\beta}$$\n\n## Population Regression Slope Vector\n\n::: callout-important\nThe **population least squares slope vector** is\n\n$$\\boldsymbol{\\beta} = (\\textbf{E}[\\mathbf{x'x}])^{-1} \\textbf{E}[\\mathbf{x}'y]$$\n:::\n\n-   Now consider pulling the intercept out of the $\\boldsymbol{\\beta}$ vector\n\n$$y = \\alpha + \\mathbf{x}\\boldsymbol{\\beta} + u$$\n\n-   Take the mean of this equation\n\n$$E[y] = E[\\alpha + \\mathbf{x}\\boldsymbol{\\beta} + u] = \\alpha + E[\\mathbf{x}]\\boldsymbol{\\beta}$$\n\n## Population Regression Slope Vector\n\n-   Subtract from first equation\n\n$$y - E[y] = (\\mathbf{x}\\boldsymbol - E[\\mathbf{x}]){\\beta}  + u$$\n\n-   Using the population linear regression vector formula\n\n$$\\boldsymbol{\\beta} = (\\textbf{E}[\\mathbf{(\\mathbf{x}\\boldsymbol - \\textbf{E}[\\mathbf{x}])'(\\mathbf{x}\\boldsymbol - \\textbf{E}[\\mathbf{x}])}])^{-1} \\textbf{E}[(\\mathbf{x}\\boldsymbol - \\textbf{E}[\\mathbf{x}])'(y - \\textbf{E}[y])]  = VAR[\\mathbf{x}]^{-1}COV[\\mathbf{x},y]$$\n\n::: callout-important\nAn alternative way to write the population least squares vector is\n\n$$\\boldsymbol{\\beta} = VAR[\\mathbf{x}]^{-1}COV[\\mathbf{x},y]$$\n\n$$\\alpha = \\textbf{E}[y] - \\textbf{E}[\\mathbf{x}]\\boldsymbol{\\beta}$$\n:::\n\n## Properties of Population Regression\n\n-   The first order condition from minimizing the MSPE by choosing $\\boldsymbol{\\beta}$ is\n\n$$\\textbf{E}[\\mathbf{x}'(y-\\mathbf{x}\\boldsymbol{\\beta})]= \\mathbf{0}$$\n\n-   This is the same as saying\n\n$$\\textbf{E}[\\mathbf{x}'u]=\\mathbf{0}$$\n\n-   Expanding that equation, we get\n\n$$\\begin{bmatrix}\n    \\textbf{E}(u)\\\\\n    \\textbf{E}(x_{1}u)\\\\\n    \\vdots\\\\\n    \\textbf{E}(x_{k}u)\n    \\end{bmatrix}\n    =\\mathbf{0}$$\n\n## Properties of Population Regression\n\n-   $\\textbf{E}[\\mathbf{x}'u]=\\mathbf{0}$ says two important things\n\n    -   The average value of the population residual $u$ is zero\n\n    -   The covariance between each $x$ and $u$ is zero\n\n-   To see the covariance part\n\n$$\\text{cov}(x_{1},u) = \\mathbf{E}[(x_{1} -  \\mathbf{E}(x_{1}))(u -  \\mathbf{E}(u))]$$\n\n-   From above, we know that $\\mathbf{E}(u) =0$, so\n\n$$\\text{cov}(x_{1},u) = \\mathbf{E}[x_{1}u -  \\mathbf{E}(x_{1})u]$$\n\n## Population Regression Slope Vector\n\n-   Bringing the expectation through the brackets\n\n$$\\text{cov}(x_{1},u) = \\mathbf{E}(x_{1}u) -  \\mathbf{E}(x_{1})\\mathbf{E}(u) = \\mathbf{E}(x_{1}u)$$\n\n-   Says that $u$ is mean zero and uncorrelated with $\\mathbf{x}$\n\n::: callout-note\n$u$ is the population residual, and is defined as $u = y - \\mathbf{x}\\boldsymbol{\\beta}$ where $\\boldsymbol{\\beta} = (\\textbf{E}[\\mathbf{x'x}])^{-1} \\textbf{E}[\\mathbf{x}'y]$\n\n**By definition** it has mean zero and is uncorrelated with $\\mathbf{x}$. We cannot use this to determine causality, which is determined by whether the slope in the CEF has a causal interpretation. We will discuss this in detail later.\n:::\n\n# Example 1: Joint Normal Variables\n\n## Linear CEF and Regression\n\n-   There are two special cases when the CEF is definitely linear\n\n    -   Joint Normal variables\n\n    -   Saturated models\n\n-   We show below that in these cases the PRF and the CEF are identical\n\n-   Note again that we have no data yet\n\n    -   We are just comparing features of the population\n\n## Joint Normal Variables\n\n-   Suppose the random variables $y$ and $x$ have a bivariate Normal distribution\n\n-   The CEF of $y$ given $x$ is\n\n$$ E[y|x] = \\mu_{y} + \\rho \\frac{\\sigma_{y}}{\\sigma_{x}}(x - \\mu_{x}) $$\n\n-   The terms in this equation are\n\n    -   $\\mu_{x}, \\mu_{y}$ are the population means of $x,y$\n    -   $\\sigma_{x}, \\sigma_{y}$ are the population standard deviations of $x,y$\n    -   $\\rho$ is the correlation coefficient between $x,y$\n\n-   This is linear in $x$ with slope $\\rho \\frac{\\sigma_{y}}{\\sigma_{x}}$\n\n## Joint Normal Variables\n\n-   Keep things simple and assume\n\n    -   $\\mu_{x} = 0$ and $\\mu_{y} = 1$\n    -   $\\sigma_{x} = 1$ and $\\sigma_{y} = 1$\n    -   $\\rho = 0.5$\n\n-   In this example the CEF is\n\n$$ E[y|x] = 1 + 0.5x $$\n\n## Distribution Plot\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){width=125%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=125%}\n:::\n:::\n\n:::\n:::\n\n\n\n## Estimating The CEF with PRF\n\n- The population regression equation to estimate this CEF would be\n\n$$y = \\alpha +  x\\beta + u$$\n\n- We derived that the slope in this regression is\n\n$$\\beta = \\frac{cov(x,y)}{var(x)}$$\n\n- From previous slide we know\n\n    - $var(x) = 1$\n    - $cov(x,y) = \\rho \\sigma_{x} \\sigma_{y} = 0.5$\n    \n- The population slope value is therefore $\\beta = 0.5$, exactly the slope of the CEF\n- The intercept is $\\alpha = \\mu_{y} - \\mu_{x}\\beta = 1$\n\n\n\n# Example 2: Saturated Model\n\n## CEF with Binary Regressor\n\n- Imagine that $y$ is a continuous variable, and $x$ takes on two values $(0,1)$\n\n- The CEF for these variables is\n\n$$E[y|x] = E[y|x = 0] + (E[y|x=1] -E[y|x=0])x$$\n$$ = \\alpha + \\beta x$$\n\n- The slope is the difference in means between the two groups\n\n\n## Population Regression Function\n\n- Again, the population regression is \n\n$$y = \\alpha +  x\\beta + u$$\n\n- Taking expectations we get\n\n$$E[y|x=0] = \\alpha + E[u|x = 0]$$\n$$E[y|x=1] = \\alpha + \\beta + E[u|x = 1]$$\n\n- The difference is\n\n$$E[y|x=1] - E[y|x=0] = \\beta + E[u|x = 1] - E[u|x = 0]$$\n\n## Population Regression Function\n\n- The last two terms are zero because of the properties of regression\n\n- To see this recall that $E[u] = 0$ in regression, and the Law of Iterated Expectations means\n\n$$E[xu] = E[xE[u|x]] = 0$$\n\n- Since $x$ takes two values, the only way for this to be true is \n\n$$ E[u|x = 1] =E[u|x = 0] = 0$$\n\n- This means \n\n$$\\beta = E[y|x=1] - E[y|x=0]$$\n\n- This is exactly the same value as the CEF\n\n\n# Example 3: Non-linear CEF\n\n## CEF\n\n- The CEF and PRF are not equal when the CEF is non-linear\n\n- Suppose that the random variable y is determined by\n\n$$y = x^2 + \\epsilon$$\n\n-  Assume the variable $x \\sim \\mathcal{N}(0, 1)$ and $\\epsilon  \\sim \\mathcal{N}(0, 1)$ and independent of $x$\n\n- The non-linear CEF in this setup is\n\n$$E[y|x] = x^2$$\n\n## Population Regression\n\n- A linear regression function would specify the relationship as\n\n$$y = \\alpha + x\\beta + e$$\n\n- We know the population slope is \n\n$$\\beta = \\frac{cov(x,y)}{var(x)}$$\n\n- Because $x \\sim \\mathcal{N}(0, 1)$ we know $var(x) = 1$\n\n- The covariance term is calculated as\n\n$$cov(x,y) = cov(x, x^2 + \\epsilon)$$\n$$=cov(x,x^2) + cov(x,\\epsilon)$$ \n\n## Population Regression\n\n\n- The second term is zero because $x$ and $\\epsilon$ are independent\n\n- For Standard Normal random variables, $x$ and $x^2$ are also uncorrelated\n\n- Based on this, the PRF slope is\n\n$$\\beta = 0$$\n\n- The population intercept is\n\n$$\\alpha = E[y] - E[x]\\beta$$\n$$=E[x^2 + \\epsilon] - E[x]\\beta$$\n$$=0$$\n\n\n## Population Regression Slope Vector\n\n-   If we observed the population we could compute $\\boldsymbol{\\beta}$\n\n-   Problem again is we do not observe the population\n\n-   So we cannot compute $\\textbf{E}[\\mathbf{x}'y]$ or $(\\textbf{E}[\\mathbf{x'x}])^{-1}$\n\n-   Instead, we collect a sample of data and estimate $\\boldsymbol{\\beta}$\n\n-   Before we do that, we briefly discuss causality in regression models\n\n# Regression and Causality\n\n## Why is Causality Important?\n\n-   Empirical economists are often interested in a **causal** effect\n\n-   For policy, it is often key to have estimate causal effect\n\n    -   E.g. a school district looking to implement pre-kindergarten program\n\n    -   This is generally funded with public money\n\n    -   Need to know if pre-k has independent effects on current and future outcomes\n\n        -   Do not want this estimate confounded with parent background\n\n-   When can we interpret a regression slope as causal?\n\n-   Answer: when the model is [structural]{.red}\n\n    -   Structural model is one where the coefficients have a causal interpretation\n\n## Model with One Binary Regressor\n\n-   In the last section we defined the underlying potential outcomes as\n\n$$y_{0} = \\alpha + \\eta$$ $$y_{1} = y_{0} + \\rho$$\n\n-   With the observed outcome\n\n$$y = \\alpha + \\rho w + \\eta$$\n\n-   This regression model is structural because $\\rho$ is the causal effect\n-   We derived that the difference in conditional expectations is\n\n$$E(y|w=1) - E(y|w=0) = \\rho  + E(\\eta |w=1) - E(\\eta |w=0)$$\n\n## Model with One Binary Regressor\n\n-   The population regression function with a binary regressor is\n\n$$y = \\beta_{0} + \\beta_{1}w + u$$\n\n-   The population least squares slope $\\beta_{1}$ from minimizing the MSPE is\n\n$$\\beta_{1} =  E(y|w=1) - E(y|w=0)$$\n\n-   Combining this equation with the structural model\n\n$$\\beta_{1}  = \\rho  + E(\\eta |w=1) - E(\\eta |w=0)$$\n\n## Model with One Binary Regressor\n\n-   The regression slope $\\beta_{1}$ equals the treatment effect $\\rho$ when\n\n    $$E(\\eta |w=1) - E(\\eta |w=0)$$\n\n-   We saw cases when this is true\n\n    -   Randomization\n\n    -   Mean independence of $\\eta$\n\n-   If none of these are true, then $\\beta_{1} \\neq \\rho$ and $\\beta_{1}$ is not a causal effect\n\n## Model with Continuous Regressor\n\n-   With a continuous independent variable $s$, suppose the structural model is\n\n$$y = \\alpha + \\rho s + \\eta$$\n\n-   Where the definition of $\\rho$ is\n\n    $$\\rho = E(y_{s_{0}}|s=s_{0}) - E(y_{s_{0}-1}|s = s_{0} - 1)$$\n\n-   Where $y_{s_{0}}$ and $y_{s_{0}-1}$ are potential outcomes with two different levels of $s$\n\n    -   $\\rho$ is the causal effect of a one-unit increase in $s$\n\n-   If we set the population regression function as\n\n$$y = \\beta_{0} + \\beta_{1}s + u$$\n\n## Model with Continuous Regressor\n\n-   The regression slope is\n\n$$\\beta_{1} = \\frac{cov(y,s)}{var(s)}$$\n\n-   To relate $\\beta_{1}$ to $\\rho$, sub in the structural model for $y$\n\n$$\\beta_{1} = \\frac{cov(\\alpha + \\rho s + \\eta ,s)}{var(s)}$$\n\n-   Simplifying we get\n\n$$\\beta_{1} = \\rho + \\frac{cov(\\eta ,s)}{var(s)}$$\n\n## Model with Continuous Regressor\n\n-   $\\beta_{1}$ equals $\\rho$ when $\\eta$ and $s$ are uncorrelated\n\n    -   Randomization, mean independence both mean this is true\n\n-   So if we assume\n\n$$E(\\eta | s) = 0$$\n\n-   Then the second term in equation above is zero and the population slope is the causal effect\n\n## Model with Continuous Regressor\n\n-   Now imagine that the structural model is\n\n$$y = \\alpha + \\rho s + \\gamma x + \\eta$$\n\n-   The definition of $\\rho$ is\n\n$$\\rho = E(y_{s_{0}}|x, s=s_{0}) - E(y_{s_{0}-1}|x, s = s_{0} - 1)$$\n\n-   If we set the population regression function as\n\n$$y = \\beta_{0} + \\beta_{1}s +  \\beta_{2} x+ u$$\n\n## Model with Continuous Regressor\n\n-   Then $\\beta_{1}$ equals $\\rho$ if we assume\n\n    -   Conditional independence of $\\eta$\n\n    -   Conditional mean independence of $\\eta$\n\n-   Conditional mean independence means\n\n$$E(\\eta | s, x) = E(\\eta | x)$$\n\n-   In words, this means $s$ is related to potential outcomes only through $x$\n\n    -   So holding $x$ constant breaks this relationship\n\n-   Even though $\\beta_{1}$ equals $\\rho$, it is important to note that $\\beta_{0} \\neq \\alpha$ and $\\beta_{2} \\neq \\gamma$\n\n    -   With regression we do not measure the structural intercept or effect of $x$\n\n## Model with Continuous Regressor\n\n-   To see this, take expectation of $y$ in structural model\n\n$$E[y|s,x] = \\alpha + \\rho s + \\gamma x + E[\\eta|s,x]$$\n\n-   If we impose conditional mean independence, then $E(\\eta | s, x) = E(\\eta | x)$\n\n$$E[y|s,x] = \\alpha + \\rho s + \\gamma x + E[\\eta|x]$$\n\n-   The error is not a function of $s$ anymore, but it is a function of $x$\n\n-   For example, suppose\n\n$$\\eta = \\theta_{0} + \\theta_{1} x + \\epsilon$$\n\n## Model with Continuous Regressor\n\n-   Assume that $\\epsilon$ is just a random error unrelated to $x$ and $s$\n\n-   Sub into structural model\n\n$$y = \\alpha + \\rho s + \\gamma x +  \\theta_{0} + \\theta_{1} x + \\epsilon$$ $$y = (\\alpha +\\theta_{0})+ \\rho s + (\\gamma + \\theta_{1})x \\epsilon$$ $$y = \\lambda + \\rho s + \\pi x + \\epsilon$$\n\n-   The intercept and slope on $x$ are now redefined\n\n    -   The are no longer causal effects\n\n-   Slope on $s$ is still the causal effect $\\rho$\n\n## Model with Continuous Regressor\n\n-   If the regression function is\n\n$$y = \\beta_{0} + \\beta_{1}s +  \\beta_{2} x+ u$$\n\n-   Then if $E[\\epsilon|s,x] = 0$\n\n$$\\beta_{0} = \\lambda$$ $$\\beta_{1} = \\rho$$ $$\\beta_{2} = \\pi$$\n\n## Omitted Variables Bias\n\n-   In the regression model above, what happens if we leave out $x$?\n\n-   Continue to assume conditional mean independence\n\n$$y = \\beta_{0} + \\beta_{1}s  + u$$\n\n-   Remember the regression slope is\n\n$$\\beta_{1} =  \\frac{cov(y ,s)}{var(s)}$$\n\n-   Sub in the structural model\n\n$$\\beta_{1} =  \\frac{cov(\\lambda + \\rho s + \\pi x + \\epsilon ,s)}{var(s)}$$\n\n## Omitted Variables Bias\n\n$$\\beta_{1} =  \\rho + \\pi* \\frac{cov( x ,s)}{var(s)} +  \\frac{cov( \\epsilon ,s)}{var(s)}$$\n\n-   The last term is zero because we assume $\\epsilon$ is unrelated to $x$ and $s$\n\n    $$\\beta_{1} =  \\rho + \\pi* \\frac{cov( x ,s)}{var(s)}$$\n\n-   The regression slope does not measure the causal effect in this case\n\n-   The bias is\n\n$$\\pi* \\frac{cov( x ,s)}{var(s)}$$\n\n## Omitted Variables Bias\n\n-   Bias has two parts\n\n    -   $\\pi \\rightarrow$ the effect of $x$ on $y$\n\n    -   $\\frac{cov( x ,s)}{var(s)} \\rightarrow$ the effect of $s$ on $x$\n\n-   If $x$ is related to $y$ [and]{.underline} $x$ is related to $s$, we have bias\n\n-   Direction of bias depends on signs of each term\n\n    -   If both positive or both negative $\\rightarrow$ positive bias\n\n    -   If one positive and one negative $\\rightarrow$ negative bias\n\n-   If either $y$ or $s$ is unrelated to $x$, there is no bias\n\n-   In vector notation, restate the structural model as\n\n$$y = \\mathbf{x_{1}}\\boldsymbol{\\alpha_{1}} + \\mathbf{x_{2}}\\boldsymbol{\\alpha_{2}} + \\eta$$\n\n## Omitted Variables Bias\n\n-   If we try to approximate it with the population regression function\n\n$$y = \\mathbf{x_{1}}\\boldsymbol{\\beta_{1}} + u$$\n\n-   We get the population regression slope as\n\n$$\\boldsymbol{\\beta_{1}}=\\left (  E[\\mathbf{x_{1}'x_{1}}\\right] )^{-1}  E[\\mathbf{x_{1}'}y]$$\n\n-   Sub the structural model into the population slope function\n\n$$\\boldsymbol{\\beta_{1}}=\\left (  E[\\mathbf{x_{1}'x_{1}}\\right] )^{-1}  E[\\mathbf{x_{1}'}( \\mathbf{x_{1}}\\boldsymbol{\\alpha_{1}} + \\mathbf{x_{2}}\\boldsymbol{\\alpha_{2}} + \\eta )]$$ $$=\\left (  E[\\mathbf{x_{1}'x_{1}}\\right] )^{-1}  E[\\mathbf{x_{1}'} \\mathbf{x_{1}}\\boldsymbol{\\alpha_{1}} + \\mathbf{x_{1}'x_{2}}\\boldsymbol{\\alpha_{2}} + \\mathbf{x_{1}'}\\eta ]$$ $$=\\left (  E[\\mathbf{x_{1}'x_{1}}\\right] )^{-1}  E[\\mathbf{x_{1}'} \\mathbf{x_{1}}]\\boldsymbol{\\alpha_{1}} + \\left (  E[\\mathbf{x_{1}'x_{1}}\\right] )^{-1}E[\\mathbf{x_{1}'x_{2}}]\\boldsymbol{\\alpha_{2}} + \\left (  E[\\mathbf{x_{1}'x_{1}}\\right] )^{-1}E[\\mathbf{x_{1}'}\\eta ]$$\n\n## Omitted Variables Bias\n\n$$=\\boldsymbol{\\alpha_{1}} + \\left (  E[\\mathbf{x_{1}'x_{1}}\\right] )^{-1}E[\\mathbf{x_{1}'x_{2}}]\\boldsymbol{\\alpha_{2}}$$\n\n-   The population slope vector on $\\mathbf{x_{1}}$ equals the sum of\n\n    -   The causal slope vector $\\boldsymbol{\\alpha_{1}}$\n\n    -   A bias term containing\n\n        -   the regression of $\\mathbf{x_{2}}$ on $\\mathbf{x_{1}}$\n\n        -   the slope on $\\mathbf{x_{2}}$ in the structural for $y$\n\n-   A key lesson here is that a single omitted variable will bias *all* population slopes $\\boldsymbol{\\beta_{1}}$\n\n    -   Unless it is unrelated to y\n\n    -   Or it is uncorrelated with all but one included regressor, and that regressor is uncorrelated with the others\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}