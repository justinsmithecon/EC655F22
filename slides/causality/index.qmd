---
format: 
  revealjs:
    theme: [default, hygge.scss]
    center-title-slide: false
    slide-number: true
    height: 900
    width: 1600
    chalkboard:
      theme: whiteboard
      src: drawings.json
editor: visual
---

<h1>Causality and Regression</h1>

<hr>

<h2>EC655 - Econometrics</h2>

<h2>Justin Smith</h2>

<h2>Wilfrid Laurier University</h2>

<h2>Fall 2023</h2>

![](/files/img/hexEC655.png){.absolute top="300" left="900" width="550"}

```{r, echo=FALSE}
library(tidyverse)
library(vtable)
library(ggthemes)
library(ggdag)
knitr::opts_chunk$set(fig.width = 4, fig.asp = 0.618, fig.align = "center",
                      fig.retina = 3, out.width = "90%", collapse = TRUE)
```

# Introduction

## Introduction

-   Empirical economists are often interested in a **causal** effect

    -   The independent effect of a particular variable on the outcome

-   It is important for policy

    -   E.g. a school district looking to implement pre-kindergarten program

    -   Need to know if pre-k has independent effects on current and future outcomes

        -   Do not want estimate confounded with parent background

-   Can we interpret regression slopes as causal?

-   First, we attempt to understand the underlying concept of causality

# Rubin Causal Model

## Model Setup

-   Economists often study causality within the **Rubin Causal Model**

-   Imagine an individual either gets a "treatment" or "no treatment"

    -   Getting a drug, or placebo

    -   Going to university, or stopping at high school

    -   Being in a small class, or large class

-   Define the following **potential outcomes**

    -   $y_{1}$ is the outcome with treatment

    -   $y_{0}$ is the outcome without treatment

    -   $w$ is a binary variable with 1 denoting treatment, and 0 no treatment

::: callout-important
We only observe one potential outcome, and the other is hypothetical. For a treated person we see $y_{1}$, and for an untreated person we see $y_{0}$
:::

## Treatment Effects

-   We would like to know the treatment effect $y_{1} - y_{0}$ for an individual

    -   This is the **causal effect** of the treatment

    -   Effect differs from person to person in the population

-   **Fundamental problem of causal inference**: we never observe both $y_{1}$ and $y_{0}$

-   We only observe $(y, w)$, where

    $$y = y_{0} + (y_{1} -y_{0})w$$

    -   We observe treatment status, potential outcome given that treatment status

-   The **counterfactual** outcome with opposite treatment is never observed

## Regression Slopes and Potential Outcomes

-   Suppose we regress $y$ on $w$

-   The slope in this regression is $\beta = E(y|w=1) - E(y|w=0)$

-   Express in terms of potential outcomes

    $$E(y|w=1) - E(y|w=0)$$ $$=  E(y_{1}|w=1) - E(y_{0}|w=0)$$ $$= \left [ E(y_{1}|w=1) - E(y_{0}|w=1) \right ] + E(y_{0}|w=1) - E(y_{0}|w=0)$$

-   The first term is called the **Average Treatment Effect on the Treated (ATT)**

    -   Average effect of the treatment for those in the treatment group

::: callout-note
Groups in the population can have different treatment effects, so there isn't necessarily a single treatment or causal effect.
:::

## Selection bias

-   The second term is **Selection Bias**

    -   Preexisting difference between treatment and control groups

-   Example: $y$ is income, and $w$ is going to university

    -   Selection bias is potential income difference in absence of university

    -   Would happen if people who end up in university are already smarter

-   If selection bias exists, regression slope is a combination of treatment effect and bias

-   There are some cases where there is no selection bias, and regression slope is only causal effect

-   We outline those below

# Randomizing Treatment Status

## Randomization and Independence of Treatment

-   A common way to isolate treatment effects is to randomize $w$

    -   Blindly put people into treatment or control group

    -   Ensures that on average the two groups are similar at baseline

-   When treatment is randomized, potential outcomes are independent from treatment $$(y_{0}, y_{1}) \perp w$$

-   Independence implies conditioning on $w$ has no effect on expectation

    $$E(y_{0}|w=1) =E(y_{0}|w=0)$$ $$E(y_{0}|w) = E(y_{0})$$ $$E(y_{1}|w) = E(y_{1})$$

## Randomization and Treatment Effects

-   With randomization, selection bias is zero

    $$E(y_{0}|w=1) - E(y_{0}|w=0) = E(y_{0}|w=1) - E(y_{0}|w=1) = 0$$

-   As a result the population regression slope is $$\beta = E(y|w=1) - E(y|w=0)$$ $$= E(y_{1}|w=1) - E(y_{0}|w=0)$$ $$= E(y_{1}) - E(y_{0})$$

-   The second term is the [Average Treatment Effect (ATE)]{.red}

::: callout-note
The ATE is the treatment effect averaged across everyone in the population, whereas the ATT is the treatment effect among only people in the treatment group (i.e. excludes people in the control group).
:::

## Recent Example in Economics Literature

-   Randomization is the standard way to measure the effects of medical treatments

-   It is becoming more popular in economics

-   Ex: Bangladesh mask study (<a href="https://www.science.org/doi/10.1126/science.abi9069">Abaluck et. al., 2021</a>)

    -   Randomized promoting mask use in rural Bangladesh

    -   Compare COVID rates between treatment and control

    -   Find some positive effect of masks, especially for age 50+

-   Next we will show how to model this in a regression framework

# Causal Effects without Randomization

## Mean Independence of Treatment

-   Most economic data do not come from randomized experiments

-   We can still uncover causal effects without experiments

-   One way is through [Mean Independence]{.red}

    $$E(y_{0}|w) = E(y_{0})$$ $$E(y_{1}|w) = E(y_{1})$$

-   Says **average** potential outcomes do not depend on treatment status

    -   Weaker assumption than full statistical independence

    -   Full independence means one event has no effect on probability of another

## Mean Independence of Treatment

-   With mean independence, we get

    $$\beta = E(y|w=1) - E(y|w=0)$$ $$= \left [ E(y_{1}|w=1) - E(y_{0}|w=1) \right ] + E(y_{0}|w=1) - E(y_{0}|w=0)$$ $$= \left [ E(y_{1}|w=1) - E(y_{0}|w=1) \right ]$$ $$=  E(y_{1}) - E(y_{0})$$

-   Regression slope equals ATE (and ATT in this case)

-   Is this assumption realistic?

    -   Means both potential outcomes unrelated to treatment

    -   Whether this is realistic depends on context

## Mean Independence of $y_{0}$

-   A variation if this assumption is [mean independence of $\mathbf{y_{0}}$]{.red}

    $$E(y_{0}|w) = E(y_{0})$$

-   The regression slope in this case is\
    $$\beta = E(y|w=1) - E(y|w=0)$$ $$= \left [ E(y_{1}|w=1) - E(y_{0}|w=1) \right ] + E(y_{0}|w=1) - E(y_{0}|w=0)$$ $$= \left [ E(y_{1}|w=1) - E(y_{0}|w=1) \right ]$$

-   With this assumption, we only measure the ATT (Not ATE)

-   Is this realistic?

    -   Means there are no baseline differences between groups on average

    -   Puts no restriction on differences in treated outcome

## Conditional Mean Independence

-   More commonly, we can use other variables to control for selection bias

-   Suppose we observe a set of pre-treatment characteristics $\mathbf{x}$

    -   Ex: gender, parental education, school test scores, etc.

    -   Key is they are determined before treatment

-   [Conditional Mean Independence]{.red} is when the mean of the potential outcomes is independent of treatment conditional on $\mathbf{x}$\
    $$E(y_{0}|w=1, \mathbf{x}) =E(y_{0}|w=0, \mathbf{x})$$ $$E(y_{0}|w, \mathbf{x}) = E(y_{0}| \mathbf{x})$$ $$E(y_{1}|w, \mathbf{x}) = E(y_{1}|\mathbf{x})$$

::: callout-note
A stronger assumption is conditional independence $(y_{0}, y_{1}) \perp w |\mathbf{x}$, where each potential outcome is fully independent of treatment conditional on $\mathbf{x}$
:::

## Conditional Mean Independence

-   Imagine running a regression of $y$ on $w$ and $\mathbf{x}$

-   The population regression slope is

    $$\beta = E(y|w=1,  \mathbf{x}) - E(y|w=0,  \mathbf{x})$$

-   When this assumption is true we can get treatment effects at each $\mathbf{x}$ $$E(y|w=1,  \mathbf{x}) - E(y|w=0,  \mathbf{x})$$ $$= E(y_{1}|w=1,  \mathbf{x}) - E(y_{0}|w=1,  \mathbf{x})= E(y_{1} | \mathbf{x}) - E(y_{0}| \mathbf{x})$$ $$= ATT( \mathbf{x}) =ATE( \mathbf{x})$$

-   These treatment effects hold $\mathbf{x}$ constant

    -   They are an average of the treatment effects across different values of $\mathbf{x}$

## Continuous Treatment

-   We can also apply this model to a continuous treatment variable

-   All of the intuition we have discussed at the same

    -   Just slightly more complex because treatment is continuous

## Summary of Rubin Model

-   Regression will identify a causal effect if

    -   Treatment comes from a randomized experiment

    -   Potential outcomes are mean independent of treatment

    -   Potential outcomes are mean independent of treatment conditional on $\mathbf{x}$

-   Without one of these, we could have selection bias

    -   Regression does not provide a causal effect

-   Unfortunately, these assumptions are generally not met

    -   We will cover other methods to uncover the causal effect in this case

# Directed Acyclic Graphs (DAG)

## Introduction

-   The Rubin framework is one way to understand causality

-   Another popular method is a **Directed Acyclic Graph (DAG)**

-   A DAG is a graphical tool to show relationships between variables

-   It can make a complex model easier to understand

-   For econometrics, it can highlight bias in our estimates

    -   And how to overcome the bias

-   In this section we will introduce DAGs, and expand on it in further sections

## Basic DAG

::: columns
::: {.column width="50%"}
-   Suppose we are interested in the direct effect of $x$ on $y$

    -   $x$ is the treatment

    -   $y$ is the outcome

-   The DAG to the right shows how $x$ and $y$ could be related

-   $z$ is another factor that is related to both $x$ and $y$

-   The dots (with variable names) are **nodes**

-   The lines connecting nodes are **edges**

-   Direction of the arrows is the direction of the relationship
:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x + z, x ~z, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::

## Basic DAG

::: columns
::: {.column width="50%"}
-   We want to find all the **paths** that can lead from $x$ to $y$

-   In this DAG there are two

    -   A direct path $x \rightarrow y$

    -   An indirect path $x \leftarrow z \rightarrow y$

-   A correlation between $y$ on $x$ would reflect both paths

    -   $x$ can directly cause $y$

    -   $z$ could change $x$ and $y$ simultaneously creating a spurious correlation between them
:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x + z, x ~z, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::

## Types of Paths

::: columns
::: {.column width="50%"}
-   In a DAG you may find two types of paths

    1.  **Front Door**: arrows point away from treatment and toward to outcome
    2.  **Back Door**: arrows point toward treatment

-   We are usually interested in front door paths

    -   Show how $x$ causes changes in $y$

-   Back door paths are usually bad

    -   Show ways that a correlation between treatment and outcome are biased
:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x + z, x ~z, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::

::: callout-caution
There can be multiple front door paths in a DAG, and we might not be interested in all of them.
:::

## Open and Closed Paths

::: columns
::: {.column width="50%"}
-   Paths between treatment and outcome can be open or closed

    1.  **Open**: all variables along the path are allowed to vary
    2.  **Closed**: at least one variable along the path cannot vary, or there is a collider along the path

-   If you regress $y$ on $x$ both paths are open

    -   $x$ and $y$ can vary, so direct path is open

    -   We are not holding $z$ fixed, so indirect path is also open

-   You can close the backdoor path by controlling for $z$

    -   Regress $y$ on $x$ **and** $z$
:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x + z, x ~z, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::

## Open and Closed Paths

::: columns
::: {.column width="50%"}
-   Suppose we are interested in the causal effect of $x$ on $y$

-   In a DAG, this means we only want the path $x \rightarrow y$

-   We need to leave this path open, but close all others

-   The regression of $y$ on $x$ and $z$ does this

-   It works in this case because $z$ is a **confounder**

-   Controlling for other variables **does not** always identify a causal effect
:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x + z, x ~z, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::

## Different Associations in DAG 

::: columns
::: {.column width="33%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x + z, x ~z, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```

<h3 style="text-align: center;">

Confounder

</h3>
:::

::: {.column width="33%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x , z ~ x, y~z ,coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```

<h3 style="text-align: center;">

Mediator

</h3>
:::

::: {.column width="33%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(z~y, z~x, y~ x, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```

<h3 style="text-align: center;">

Collider

</h3>
:::
:::



## Confounder

::: columns
::: {.column width="50%"}
-   The DAG we have been working with has $z$ as a confounder

- **Confounder**: The treatment and outcome have a common cause

- In this example

    - $z$ changes $x$ and $y$ simultaneously
    
    - Makes it look like $x$ and $y$ are related, but it is spurious
    
- Example: Returns to Schooling

    - $y$ is wages, $x$ is years of schooling
    
    - A confounder $z$ would be intelligence
    
- Can close the backdoor path by controlling for $z$
:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x + z, x ~z, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::


## Mediator

::: columns
::: {.column width="50%"}
-   In this DAG we have two paths

$$x \rightarrow y$$
$$x \rightarrow z \rightarrow y$$ 

- Both are front door paths

- In this example $z$ acts as a mediator

- **Mediator**: A variable within a causal pathway between treatment and outcome

- In this example

    - $x$ causes $y$ directly
    
    - But also indirectly through $z$


:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x , z ~ x, y~z ,coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::


## Mediator

::: columns
::: {.column width="50%"}
-   This is not the same as omitted variables bias

- A regression of $y$ on $x$ measures the causal effect

    - Two causal paths between $x$ and $y$

- We can still close the indirect path by controlling for $z$

- This would leave only the direct path

- Whether we want to do this depends on the research question

    - If you want only direct effect, control for $z$
    
    - For total effect, do not control

:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x , z ~ x, y~z ,coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::

## Mediator

::: columns
::: {.column width="50%"}
-   Example: Effect of drinking on lifespan

- $x$ is drinking, $y$ is lifespan, $z$ is drug use

- If drug use is a mediator then

$$drinking \rightarrow lifespan$$
$$drinking \rightarrow drugs \rightarrow lifespan$$

- Drinking has direct effect on lifespan, and also through drug use

- Regression of $lifespan$ on $drinking$ measures total effect

- Regression of $lifespan$ on $drinking$ and $drug$ use measures only direct effect

:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(y~x , z ~ x, y~z ,coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::


## Collider

::: columns
::: {.column width="50%"}
- Final association is trickier

- **Collider**: a variable that is influenced by two or more variables

- In this DAG, $z$ is a collider

    - Both $y$ and $x$ point to it
    
- Paths that contain a collider are closed

- In this case, you could regress $y$ on $x$ and get causal effect

:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(z~y, z~x, y~ x, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::


## Collider

::: columns
::: {.column width="50%"}
- Controlling for a collider opens the path

- Regression of $y$ on $x$ and $z$ does not measure causal effect

- **Collider Bias**: bias induced by controlling for collider

- Collider bias can

    - Create an association between $x$ and $y$ when there is no direct one
    
    - Hide a direct association
    
-  Easiest way to understand is through example

:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(z~y, z~x, y~ x, coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::


## Collider

::: columns
::: {.column width="50%"}
- Example: talent and effort

- $x$ is talent, $y$ is effort, $z$ is being in elite school

- Assume no direct link between talent and effort

- Both talented and hard working students make it to elite schools

- Controlling for school, talent and effort look negatively related

    - You need to be smart or work hard to get into Harvard
    
    - Low-effort students must be smart, and un-smart students must be hard working
    

:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(z~y, z~x,  coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::

## Collider

::: columns
::: {.column width="50%"}
- Example: gender pay gap

- $x$ is gender, $y$ is ability, $z$ is occupation

- Assume no direct link between gender and ability

- High ability people make it into certain occupations

- If discrimination against women, they might also be selected into those jobs

- Controlling for occupation, gender and ability look negatively related

- This could lead to gender pay gap being larger within jobs
    

:::

::: {.column width="50%"}
```{r echo = FALSE, fig.asp = .8}
coord_dag<-list(x = c(z = 1, x = 0, y = 2), y = c(z = 0, x = 1, y = 1))
dag <- dagify(z~y, z~x,  coords = coord_dag) %>% 
  tidy_dagitty() %>% mutate(linetype = ifelse(name == "z", "dashed", "solid"))

ggplot(dag,aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() 
```
:::
:::


# Simulation Examples

## Data Setup {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   To help understand the Rubin model we will demonstrate with simulated data

-   Code to the right creates potential outcomes

-   For simplicity the treatment effect is set to 5 for everyone

-   Outcomes $y_{0}$ and $y_{1}$ have a Normal distribution
:::

::: {.column width="50%"}
```{r, echo=TRUE}
data <- data.frame(eta=rnorm(100000,0,1)) %>%
  mutate(y0 = 2 + eta, y1 = y0 + 5, 
         treat_eff = y1 - y0)

sumtable(data, summ=c('notNA(x)','mean(x)','min(x)', 'max(x)'), summ.names = c('N', 'Mean', 'Min', 'Max' ))
```
:::
:::

## Random Assignment to Treatment {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Next assign treatment $w$ using [randomization]{.red}

-   In the code, $w=1$ randomly with probability 0.5

-   Compute observed $y$ based on treatment status
:::

::: {.column width="50%"}
```{r, echo=TRUE}
data %<>% mutate(w = if_else(runif(100000) > .5,1,0), 
                 y = y0 + (y1-y0)*w) %>% 
  group_by(w)

head(data)
```
:::
:::

## Random Assignment to Treatment {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   With random assignment we know

    -   $y_{0}$ is independent of $w$

    -   $y_{1}$ is independent of $w$

-   So the distributions of $y_{0}$ and $y_{1}$ are the same when $w=0$ and when $w=1$

-   To the right we show the distribution of $y_{0}$
:::

::: {.column width="50%"}
```{r, echo=TRUE}
ggplot(data, aes(x=y0, color=as.factor(w))) +
  geom_density(alpha = .4, size=2) +
  theme_pander(nomargin=FALSE, boxes=TRUE) +
  labs(title = "Distribution of Y0")
```
:::
:::

## Random Assignment to Treatment {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Randomization ensures difference in average $y$ between groups equals the [ATE and ATT]{.red}

-   On the right we show the difference in mean of $y$ equals 5

    -   (it's not exactly 5 due to sampling error)
:::

::: {.column width="50%"}
```{r, echo=TRUE}
summarize(data, mean(y))
summarize(data,mean(y))$`mean(y)`[2] - 
  summarize(data,mean(y))$`mean(y)`[1]
```
:::
:::

## Random Assignment to Treatment {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Can implement difference in means as a regression

-   Recall slope in OLS regression of $y$ on dummy variable is difference in means of $y$
:::

::: {.column width="50%"}
```{r, echo=TRUE}
lm(y ~ w, data)
```
:::
:::

## Selection into Treatment {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Now simulate selection into treatment based on $y_{0}$

    -   Treatment now related to value of $y_{0}$

-   We know $\eta$ determines value of $y_{0}$

    -   If we make $w=$ with higher values of $\eta$ then $w$ is related to $y_{0}$
:::

::: {.column width="50%"}
```{r, echo=TRUE, results=FALSE}
data2 <- data %>% 
  ungroup() %>% 
  select(eta, y0,y1) %>%
  mutate(w = if_else(eta + runif(100000,-1,1) > 0,1,0), 
         y = y0 + (y1-y0)*w) %>%
  group_by(w)

sumtable(data2, 
         summ=c('notNA(x)','mean(x)','sd(x)'), 
         group="w",
         group.long = TRUE)
```
:::
:::

## Selection into Treatment {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   The means of $y_{0}$ and $y_{1}$ are now different by group

-   Because of selection bias

    -   Treated group has better non-treated outcomes
:::

::: {.column width="50%"}
```{r, results=TRUE}
sumtable(data2, 
         summ=c('notNA(x)','mean(x)','sd(x)'), 
         group="w",
         group.long = TRUE)
```
:::
:::

## Selection into Treatment {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   The distribution of $y_{0}$ differs by $w$

    -   Treated group has better baseline outcomes

-   This creates selection bias
:::

::: {.column width="50%"}
```{r, echo=TRUE}
ggplot(data2, aes(x=y0, color=as.factor(w))) +
  geom_density(alpha = .4, size=2) +
  theme_pander(nomargin=FALSE, boxes=TRUE) +
  labs(title = "Distribution of Y0")
```
:::
:::

## Selection into Treatment {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Selction bias shows up when you take difference in mean $y$

    -   We know the true treatment effect is 5

    -   But difference in $y$ is larger

    -   There is positive selection bias

    -   Bias is about 1.4
:::

::: {.column width="50%"}
```{r, echo=TRUE}
summarize(data2, mean(y))
summarize(data2,mean(y))$`mean(y)`[2] - 
  summarize(data2,mean(y))$`mean(y)`[1]
```
:::
:::

## Selection into Treatment {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   You can implement this as a regression

-   OLS estimates biased treatment effect

-   Remember the intercept is mean of $y$ when $w=0$
:::

::: {.column width="50%"}
```{r, echo=TRUE}
lm(y ~ w, data2)
```
:::
:::

## Mean Independence of $y_{0}$ {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Randomization ensures entire distribution of $y_{0}$ is the same for treatment and control

-   We do not need this to estimate **average** treatment effects

-   If the **mean** of $y_{0}$ is the same between treatment and control we can estimate treatment effect with difference in means

-   Code makes mean of $y_{0}$ the same, but variance bigger for $w=1$
:::

::: {.column width="50%"}
```{r, echo=TRUE, results=FALSE}
data3 <- data %>% 
  ungroup() %>% 
  select(eta, y0,y1) %>%
  mutate(w =  if_else(between(percent_rank(y0),.25,.75),0,1), 
         y = y0 + (y1-y0)*w) %>%
  group_by(w)

sumtable(data3, 
         summ=c('notNA(x)','mean(x)','sd(x)'), 
         group="w",
         group.long = TRUE)
```
:::
:::

## Mean Independence of $y_{0}$ {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Randomization ensures entire distribution of $y_{0}$ is the same for treatment and control

-   We do not need this to estimate **average** treatment effects

-   If the **mean** of $y_{0}$ is the same between treatment and control we can estimate treatment effect with difference in means

-   Code makes mean of $y_{0}$ the same, but variance bigger for $w=1$
:::

::: {.column width="50%"}
```{r, results=TRUE}
sumtable(data3, 
         summ=c('notNA(x)','mean(x)','sd(x)'), 
         group="w",
         group.long = TRUE)
```
:::
:::

## Mean Independence of $y_{0}$ {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   The distribution of $y_{0}$ is plotted on the right

-   The spread is larger for $w=1$

-   This does not affect estimate of the **average** treatment effect
:::

::: {.column width="50%"}
```{r, echo=TRUE, fig.height = 4.55}
ggplot(data3, aes(x=y0, color=as.factor(w))) +
  geom_density(alpha = .4, size=2) +
  theme_pander(nomargin=FALSE, boxes=TRUE) +
  labs(title = "Distribution of Y0")
```
:::
:::

## Mean Independence of $y_{0}$ {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Take difference in mean $y$

    -   This equals the treatment effect

    -   Difference in variance did not create bias
:::

::: {.column width="50%"}
```{r, echo=TRUE}
summarize(data2, mean(y))
summarize(data2,mean(y))$`mean(y)`[2] - 
  summarize(data2,mean(y))$`mean(y)`[1]
```
:::
:::

## Mean Independence of $y_{0}$ {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Running regression produces same result

-   The variance in $y_{1}$ affects the standard error

    -   But we are not concerned with that right now
:::

::: {.column width="50%"}
```{r, echo=TRUE}
lm(y ~ w, data3)
```
:::
:::

## Conditional Mean Independence {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Finally consider [conditional mean independence]{.red}

-   Treatment is related to $y_{0}$, but only through $x$

    -   For people with the same $x$, $y_{0}$ is unrelated to $w$

-   Ex: Education and wages

    -   Smart people $(x = 1)$ earn higher wages regardless of schooling $(y_0)$

    -   Smart people are more likely to go to university $(w = 1)$

    -   People at university will have higher $y_0$
:::

::: {.column width="50%"}
```{r, echo=TRUE, results=FALSE}
data4 <- data %>% 
  ungroup() %>% 
  select(eta) %>%
  mutate(x = if_else(runif(100000) > .5,1,0),
         w = if_else(x + runif(100000, -1,1) > .5,1,0),
         y0 = 2 + 3*x + eta,
         y1 = y0 + 5,
         y = y0 + (y1-y0)*w) %>%
  group_by(w)

sumtable(data4, 
         summ=c('notNA(x)','mean(x)','sd(x)'), 
         group="w",
         group.long = TRUE)
```
:::
:::

## Conditional Mean Independence {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Comparing treatment and control, $y_{0}$ is bigger when $w=1$

-   This is because

    -   $y_{0}$ is bigger when $x=1$

    -   $w$ more likely to be $1$ when $x=1$
:::

::: {.column width="50%"}
```{r, results=TRUE}
sumtable(data4, 
         summ=c('notNA(x)','mean(x)','sd(x)'), 
         group="w",
         group.long = TRUE)
```
:::
:::

## Conditional Mean Independence {background-color="#d9edc2"}

::: columns
::: {.column width="35%"}
-   What if we focus only on people with $x=1$?

-   No difference in $y_{0}$ between treated and control

    -   Because $x$ is only reason why they differed

    -   This is holding $x$ fixed
:::

::: {.column width="65%"}
```{r, results=TRUE, echo= TRUE}
sumtable(filter(data4, x==1), 
         summ=c('notNA(x)','mean(x)','sd(x)'), 
         group="w")
```
:::
:::

## Conditional Mean Independence {background-color="#d9edc2"}

::: columns
::: {.column width="35%"}
-   Same result if we hold $x=0$?

    -   Again because $x$ is only reason why they differed
:::

::: {.column width="65%"}
```{r, results=TRUE, echo= TRUE}
sumtable(filter(data4, x==0), 
         summ=c('notNA(x)','mean(x)','sd(x)'), 
         group="w")
```
:::
:::

## Conditional Mean Independence {background-color="#d9edc2"}

::: columns
::: {.column width="50%"}
-   Regression of $y$ on $w$ is biased

    -   Because $w$ is correlated with error

-   But regression of $y$ on $w$ and $x$ generates actual treatment effect

-   This is conditional mean independence

    -   Holding $x$ fixed, potential outcomes no longer related to treatment
:::

::: {.column width="50%"}
```{r, echo=TRUE}
lm(y ~ w, data4)
lm(y ~ w + x, data4)
```
:::
:::

# Introduction

## Model with One Binary Regressor

-   In the last section we defined the underlying potential outcomes as

$$y_{0} = \alpha + \eta$$ $$y_{1} = y_{0} + \rho$$

-   With the observed outcome

$$y = \alpha + \rho w + \eta$$

-   This regression model is structural because $\rho$ is the causal effect
-   We derived that the difference in conditional expectations is

$$E(y|w=1) - E(y|w=0) = \rho  + E(\eta |w=1) - E(\eta |w=0)$$

## Model with One Binary Regressor

-   The population regression function with a binary regressor is

$$y = \beta_{0} + \beta_{1}w + u$$

-   The population least squares slope $\beta_{1}$ from minimizing the MSPE is

$$\beta_{1} =  E(y|w=1) - E(y|w=0)$$

-   Combining this equation with the structural model

$$\beta_{1}  = \rho  + E(\eta |w=1) - E(\eta |w=0)$$

## Model with One Binary Regressor

-   The regression slope $\beta_{1}$ equals the treatment effect $\rho$ when

    $$E(\eta |w=1) - E(\eta |w=0)$$

-   We saw cases when this is true

    -   Randomization

    -   Mean independence of $\eta$

-   If none of these are true, then $\beta_{1} \neq \rho$ and $\beta_{1}$ is not a causal effect

## Model with Continuous Regressor

-   With a continuous independent variable $s$, suppose the structural model is

$$y = \alpha + \rho s + \eta$$

-   Where the definition of $\rho$ is

    $$\rho = E(y_{s_{0}}|s=s_{0}) - E(y_{s_{0}-1}|s = s_{0} - 1)$$

-   Where $y_{s_{0}}$ and $y_{s_{0}-1}$ are potential outcomes with two different levels of $s$

    -   $\rho$ is the causal effect of a one-unit increase in $s$

-   If we set the population regression function as

$$y = \beta_{0} + \beta_{1}s + u$$

## Model with Continuous Regressor

-   The regression slope is

$$\beta_{1} = \frac{cov(y,s)}{var(s)}$$

-   To relate $\beta_{1}$ to $\rho$, sub in the structural model for $y$

$$\beta_{1} = \frac{cov(\alpha + \rho s + \eta ,s)}{var(s)}$$

-   Simplifying we get

$$\beta_{1} = \rho + \frac{cov(\eta ,s)}{var(s)}$$

## Model with Continuous Regressor

-   $\beta_{1}$ equals $\rho$ when $\eta$ and $s$ are uncorrelated

    -   Randomization, mean independence both mean this is true

-   So if we assume

$$E(\eta | s) = 0$$

-   Then the second term in equation above is zero and the population slope is the causal effect

## Model with Continuous Regressor

-   Now imagine that the structural model is

$$y = \alpha + \rho s + \gamma x + \eta$$

-   The definition of $\rho$ is

$$\rho = E(y_{s_{0}}|x, s=s_{0}) - E(y_{s_{0}-1}|x, s = s_{0} - 1)$$

-   If we set the population regression function as

$$y = \beta_{0} + \beta_{1}s +  \beta_{2} x+ u$$

## Model with Continuous Regressor

-   Then $\beta_{1}$ equals $\rho$ if we assume

    -   Conditional independence of $\eta$

    -   Conditional mean independence of $\eta$

-   Conditional mean independence means

$$E(\eta | s, x) = E(\eta | x)$$

-   In words, this means $s$ is related to potential outcomes only through $x$

    -   So holding $x$ constant breaks this relationship

-   Even though $\beta_{1}$ equals $\rho$, it is important to note that $\beta_{0} \neq \alpha$ and $\beta_{2} \neq \gamma$

    -   With regression we do not measure the structural intercept or effect of $x$

## Model with Continuous Regressor

-   To see this, take expectation of $y$ in structural model

$$E[y|s,x] = \alpha + \rho s + \gamma x + E[\eta|s,x]$$

-   If we impose conditional mean independence, then $E(\eta | s, x) = E(\eta | x)$

$$E[y|s,x] = \alpha + \rho s + \gamma x + E[\eta|x]$$

-   The error is not a function of $s$ anymore, but it is a function of $x$

-   For example, suppose

$$\eta = \theta_{0} + \theta_{1} x + \epsilon$$

## Model with Continuous Regressor

-   Assume that $\epsilon$ is just a random error unrelated to $x$ and $s$

-   Sub into structural model

$$y = \alpha + \rho s + \gamma x +  \theta_{0} + \theta_{1} x + \epsilon$$ $$y = (\alpha +\theta_{0})+ \rho s + (\gamma + \theta_{1})x \epsilon$$ $$y = \lambda + \rho s + \pi x + \epsilon$$

-   The intercept and slope on $x$ are now redefined

    -   The are no longer causal effects

-   Slope on $s$ is still the causal effect $\rho$

## Model with Continuous Regressor

-   If the regression function is

$$y = \beta_{0} + \beta_{1}s +  \beta_{2} x+ u$$

-   Then if $E[\epsilon|s,x] = 0$

$$\beta_{0} = \lambda$$ $$\beta_{1} = \rho$$ $$\beta_{2} = \pi$$

## Omitted Variables Bias

-   In the regression model above, what happens if we leave out $x$?

-   Continue to assume conditional mean independence

$$y = \beta_{0} + \beta_{1}s  + u$$

-   Remember the regression slope is

$$\beta_{1} =  \frac{cov(y ,s)}{var(s)}$$

-   Sub in the structural model

$$\beta_{1} =  \frac{cov(\lambda + \rho s + \pi x + \epsilon ,s)}{var(s)}$$

## Omitted Variables Bias

$$\beta_{1} =  \rho + \pi* \frac{cov( x ,s)}{var(s)} +  \frac{cov( \epsilon ,s)}{var(s)}$$

-   The last term is zero because we assume $\epsilon$ is unrelated to $x$ and $s$

    $$\beta_{1} =  \rho + \pi* \frac{cov( x ,s)}{var(s)}$$

-   The regression slope does not measure the causal effect in this case

-   The bias is

$$\pi* \frac{cov( x ,s)}{var(s)}$$

## Omitted Variables Bias

-   Bias has two parts

    -   $\pi \rightarrow$ the effect of $x$ on $y$

    -   $\frac{cov( x ,s)}{var(s)} \rightarrow$ the effect of $s$ on $x$

-   If $x$ is related to $y$ [and]{.underline} $x$ is related to $s$, we have bias

-   Direction of bias depends on signs of each term

    -   If both positive or both negative $\rightarrow$ positive bias

    -   If one positive and one negative $\rightarrow$ negative bias

-   If either $y$ or $s$ is unrelated to $x$, there is no bias

-   In vector notation, restate the structural model as

$$y = \mathbf{x_{1}}\boldsymbol{\alpha_{1}} + \mathbf{x_{2}}\boldsymbol{\alpha_{2}} + \eta$$

## Omitted Variables Bias

-   If we try to approximate it with the population regression function

$$y = \mathbf{x_{1}}\boldsymbol{\beta_{1}} + u$$

-   We get the population regression slope as

$$\boldsymbol{\beta_{1}}=\left (  E[\mathbf{x_{1}'x_{1}}\right] )^{-1}  E[\mathbf{x_{1}'}y]$$

-   Sub the structural model into the population slope function

$$\boldsymbol{\beta_{1}}=\left (  E[\mathbf{x_{1}'x_{1}}\right] )^{-1}  E[\mathbf{x_{1}'}( \mathbf{x_{1}}\boldsymbol{\alpha_{1}} + \mathbf{x_{2}}\boldsymbol{\alpha_{2}} + \eta )]$$ $$=\left (  E[\mathbf{x_{1}'x_{1}}\right] )^{-1}  E[\mathbf{x_{1}'} \mathbf{x_{1}}\boldsymbol{\alpha_{1}} + \mathbf{x_{1}'x_{2}}\boldsymbol{\alpha_{2}} + \mathbf{x_{1}'}\eta ]$$ $$=\left (  E[\mathbf{x_{1}'x_{1}}\right] )^{-1}  E[\mathbf{x_{1}'} \mathbf{x_{1}}]\boldsymbol{\alpha_{1}} + \left (  E[\mathbf{x_{1}'x_{1}}\right] )^{-1}E[\mathbf{x_{1}'x_{2}}]\boldsymbol{\alpha_{2}} + \left (  E[\mathbf{x_{1}'x_{1}}\right] )^{-1}E[\mathbf{x_{1}'}\eta ]$$

## Omitted Variables Bias

$$=\boldsymbol{\alpha_{1}} + \left (  E[\mathbf{x_{1}'x_{1}}\right] )^{-1}E[\mathbf{x_{1}'x_{2}}]\boldsymbol{\alpha_{2}}$$

-   The population slope vector on $\mathbf{x_{1}}$ equals the sum of

    -   The causal slope vector $\boldsymbol{\alpha_{1}}$

    -   A bias term containing

        -   the regression of $\mathbf{x_{2}}$ on $\mathbf{x_{1}}$

        -   the slope on $\mathbf{x_{2}}$ in the structural for $y$

-   A key lesson here is that a single omitted variable will bias *all* population slopes $\boldsymbol{\beta_{1}}$

    -   Unless it is unrelated to y

    -   Or it is uncorrelated with all but one included regressor, and that regressor is uncorrelated with the others
